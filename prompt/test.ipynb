Import all dependencies
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import evaluate

Load the CNN dataset (Data source)
dataset = load_dataset("cnn_dailymail", "3.0.0")
sample = dataset['test'][1]

article = sample['article']
reference_summary = sample['highlights']

print("ARTICLE:", article)
print("REFERENCE:", reference_summary)
Generate Summary
model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

prompt = "Summarize the following news article:\n" + article

inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
outputs = model.generate(inputs.input_ids, max_new_tokens=200)

summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Zero Shot Summary:\n", summary)
Evaluate (ROUGE)
rouge = evaluate.load("rouge")
results = rouge.compute(predictions=[summary], references=[reference_summary])
print("\nROUGE SCORE:", results)
