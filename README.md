# Prompt Summarization Evolution

This repository is part of my **Research Project 7100A** at the **University of Adelaide**.  
It investigates how different **prompt engineering strategies** and **mutation-based optimization methods** influence the quality of text summarization using **T5-based large language models (LLMs)**.

## ğŸ“ Project Structure

prompt-summarization/<br>
â”‚<br>
â”œâ”€â”€ main.py # Entry script for evolutionary summarization<br>
â”œâ”€â”€ evolution.py # Implements prompt evolution across multiple rounds<br>
â”œâ”€â”€ mutations.py # Defines mutation strategies for prompts<br>
â”œâ”€â”€ baseline_generate.py # Runs baseline (no prompt/mutation) summarization<br>
â”œâ”€â”€ visualize_results.py # Visualization of results (bar/line charts)<br>
â”œâ”€â”€ evaluation.py # Calculates ROUGE-1, ROUGE-L, FRE, compression<br>
â”œâ”€â”€ llm_utils.py # Model wrapper (T5 query / decoding utilities)<br>
â”œâ”€â”€ data_utils.py # Helper for loading and preprocessing datasets<br>
â”œâ”€â”€ sample_extraction.py # Selects articles from CNN and XSum datasets<br>
â”œâ”€â”€ mini-demo.py # Minimal interactive demo (two prompts comparison)<br>
â”‚<br>
â”œâ”€â”€ data/<br>
â”‚ â”œâ”€â”€ cnn_input.json # CNN/DailyMail test samples<br>
â”‚ â””â”€â”€ xsum_input.json # XSum test samples<br>
â”‚<br>
â”œâ”€â”€ results/ # All experimental outputs and figures<br>
â”‚ â”œâ”€â”€ round_*.json # Per-round summarization results<br>
â”‚ â”œâ”€â”€ meet_threshold.json # Prompts meeting threshold across rounds<br>
â”‚ â”œâ”€â”€ baseline.json # Baseline summarization results<br>
â”‚ â””â”€â”€ ... # Other visualization charts<br>
â”‚<br>
â”œâ”€â”€ requirements.txt # Dependencies<br>
â””â”€â”€ README.md<br>

## Current Progress
### 1. Prompting Strategies
Five styles are compared:
- zero_shot
- few_shot
- instruction_based
- pattern_based
- target_audience

### 2. Datasets
- **CNN/DailyMail** â€” long-form news articles  
- **XSUM** â€” short single-sentence summaries  

### 3. Mutation Methods
Each prompt undergoes up to five mutation types:
- Synonym replacement  
- Prompt rewriting  
- Style instruction  
- Audience information  
- Stepwise (multi-step) prompts  

### 4. Evaluation Metrics
Each generated summary is automatically evaluated using:
- **ROUGE-1**  
- **ROUGE-L**  
- **Flesch Reading Ease (FRE)**  
- **Compression ratio**

### 5. Baseline vs Evolution Comparison
- `baseline_generate.py`: Runs summarization without any prompt/mutation  
- `visualize_results.py`: Compares baseline vs final evolved prompts (bar/line charts)

### Setup

1. **Clone the repository**
   git clone https://github.com/JiyinShao/prompt-summarization-demo-.git
   cd prompt-summarization-demo-
2. **Install dependencies**
   pip install -r requirements.txt
3. **Run the summarization and evaluation pipeline**
   python evolution.py
4. **Aggregate results & generate charts**
   python visualize_results.py

This will:
 - Load 20 test samples from each dataset
 - Apply all five prompt templates
 - Generate summaries using T5
 - Score outputs with ROUGE-1 / ROUGE-L / FRE / Compression Rate
 - Save results under results/
 - Export comparison charts 

## Contact

Author: **Jiyin Shao**  
Email: [a1903968@adelaide.edu.au]  
University of Adelaide, Research Project 7100A
